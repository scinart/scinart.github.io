---
layout: post
title: "CS 7545 Fall 2018 Note"
date: 2018-08-26
timestamp: "2018-09-05 13:17:06 mama"
categories: GT
comments: true
cc: "by-nc-nd"

---

#### From Foundation of Machine Learning (Mohri)

+ A.0 Notation

  We will denote by $$ \mathbb{H} $$ a vector space whose dimension may be infinite.

+ A.1 Definition ***Norms***

  A mapping $$ \| \cdot \| : \mathbb{H} \mapsto \mathbb{R}^+ $$ is said to define a norm if:

  * definiteness: $$ \| \textbf{x} \| = 0 \iff \textbf{x} = 0 $$
  * homogeneity: $$ \forall x \in \mathbb{H}, \forall \alpha \in \mathbb{R},\, \| alp \textbf{x} \| = \left\vert \alpha \right\vert \| \textbf{x} \| $$ 
  * triangle inequality: $$ \forall \textbf{x}, \textbf{y} \in \mathbb{H}, \| \textbf{x} + \textbf{y} \| \leq \| \textbf{x} \| + \| \textbf{y} \| $$.

  My comment: according to [Wikipedia](https://en.wikipedia.org/wiki/Norm_(mathematics)). $$ \mathbb{H} $$ **must be a vector space** over a **subfield** of $$ \mathbb{C} $$. This is required for $$ \left\vert \alpha \right\vert $$ to make sense ( or somehow $$ \mathbb{H} $$ is endowed with an absolute value ) . In **homogeneity** it should also be $$ \alpha \in \mathbb{F} $$ where $$ \mathbb{F} $$ is a subfield of $$ \mathbb{C} $$.

+ p-norm 

  for $$ p \geq 1 $$, the ***p-norm*** of $$ \textbf{x} \in \mathbb{C}^n $$ is defined as

  $$ \| \textbf{x} \|_p = \left( \sum_{j=1}^{n} \left\vert x_j \right\vert ^p \right) ^{1/p} $$

+ equivalency of norms

  two norms $$ \| \cdot \| $$ and $$ \| \cdot \|' $$ are said to be equivalent iff there exists real number $$\alpha, \beta \gt 0 $$ such that $$ \forall \textbf{x} $$

  $$ \alpha \| \textbf{x} \| \leq \| \textbf{x} \|' \leq \beta \| \textbf{x} \| $$

  More generally, all norms on a finite-dimensional space are equivalent.

  My comment: **To be specific**: all norms on a finite-dimensional **Banach** space are equivalent. [Proof](https://math.mit.edu/~stevenj/18.335/norm-equivalence.pdf) ([Cached](/pdfs/norm-equivalence.pdf)). I cannot find for now a counter example of a non-equivalent norm on a non-complete finite-dimensional space.

+ Dual norms

  [see intuition of dual norm here](https://math.stackexchange.com/questions/903484/dual-norm-intuition)([cached](https://web.archive.org/web/20180828200803/https://math.stackexchange.com/questions/903484/dual-norm-intuition))

  Dual is a norm on the dual space

  Let $$ f : V \mapsto \mathbb{F} $$, 

  $$ \| f \|_* = \sup_{x\neq 0}{\frac{ \left\vert f(x) \right\vert} {\| x \|}} $$

  This actually belongs to functional analysis... I don't know...

#### Notes about Fenchel Conjugate

  TODO

#### Notes about some probability.

+ Markov inequality

  $$ P(X\geq a) \leq \frac{\mathbb{E}[x]}{a} $$

  In the language of measure theory, Markov's inequality states that if $$ (X, \Sigma, \mu) $$ is a measure space, f is a measurable extended real-valued function, and $$ \epsilon > 0 $$, then

  $$ μ ( \{ x ∈ X : \left\vert f ( x ) \right\vert ≥ ε \} ) ≤ \frac{1}{\epsilon} \int_X { \left\vert f \right\vert \operatorname{d\mu} }. $$
  
  If $$ \varphi $$ is a monotonically increasing nonnegative function for the nonnegative reals, $$ X $$ is a random variable, $$ a ≥ 0 $$, and $$ \varphi(a) > 0 $$, then

  $$ P ( \left\vert X \right\vert \geq a ) \leq \frac{\mathbb{E}[\varphi(\left\vert X \right\vert )]}{\varphi(a)} $$

  An immediately corollary, using higher moments of **nonnegative** $$X$$ is

  $$ P (X \geq a) \leq \frac{\mathbb{E}[X^n]}{a^n} $$

+ Chebyshev's inequality

  Chebyshev's inequality uses the variance to bound the probability that a random variable deviates far from the mean. Specifically: 

  $$ P( \left\vert X - \mathbb{E}[X] \right\vert \geq a ) \leq \frac{\operatorname{Var}(X)}{a^2},\,\, a > 0 $$

  for which Markov's inequality reads

  $$ {\displaystyle \operatorname {P} \left((X- \mathbb{E}[X])^{2}\geq a^{2}\right)\leq {\frac {\operatorname {Var} (X)}{a^{2}}},} $$

+ Hoeffding's Lemma

  TODO: **too long, will write later.**

+ Hoeffding's_inequality

  Let $$ X_1, \ldots, X_n $$ be independent random variables bounded by the interval $$ [0, 1]: 0 ≤ X_i ≤ 1 $$. We define the empirical mean of these variables by

  $$ {\displaystyle {\overline {X}}={\frac {1}{n}}(X_{1}+\cdots +X_{n}).} $$

  One of the inequalities in Theorem 1 of Hoeffding (1963) states

  $$ {\displaystyle {\begin{aligned}\operatorname {P} ({\overline {X}}-\mathrm {E} [{\overline {X}}]\geq t)\leq e^{-2nt^{2}}\end{aligned}}} $$

  where $$ {\displaystyle 0\leq t}. $$

  Theorem 2 of Hoeffding (1963) is a generalization of the above inequality when it is known that $$ X_i $$ are strictly bounded by the intervals $$ [a_i, b_i] $$:

  $$ {\displaystyle {\begin{aligned}\operatorname {P} \left({\overline {X}}-\mathrm {E} \left[{\overline {X}}\right]\geq t\right)&\leq \exp \left(-{\frac {2n^{2}t^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right)\\\operatorname {P} \left(\left|{\overline {X}}-\mathrm {E} \left[{\overline {X}}\right]\right|\geq t\right)&\leq 2\exp \left(-{\frac {2n^{2}t^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right)\end{aligned}}} $$

+ Proof

  * Hoeffding's Lemma:
    Suppose $$ X $$ is a real random variable with mean zero such that $$ {\displaystyle \textstyle \operatorname {P} \left(X\in \left[a,b\right]\right)=1} $$.
    Then $$ {\displaystyle \mathrm {E} \left[e^{sX}\right]\leq \exp \left({\tfrac {1}{8}}s^{2}(b-a)^{2}\right).} $$
    Using this lemma, we can prove Hoeffding's inequality.

    Suppose $$ X_1,\ldots,X_n $$ are n independent random variables such that

    $$ {\displaystyle \operatorname {P} \left(X_{i}\in [a_{i},b_{i}]\right)=1,\qquad 1\leq i\leq n.} $$

    Let $$ {\displaystyle S_{n}=X_{1}+\cdots +X_{n}.} $$

    Then for $$ s, t ≥ 0 $$, Markov's inequality and the independence of $$X_i$$ implies:

    $$ {\displaystyle {\begin{aligned}\operatorname {P} \left(S_{n}-\mathrm {E} \left[S_{n}\right]\geq t\right)&=\operatorname {P} \left(e^{s(S_{n}-\mathrm {E} \left[S_{n}\right])}\geq e^{st}\right)\\&\leq e^{-st}\mathrm {E} \left[e^{s(S_{n}-\mathrm {E} \left[S_{n}\right])}\right]\\&=e^{-st}\prod _{i=1}^{n}\mathrm {E} \left[e^{s(X_{i}-\mathrm {E} \left[X_{i}\right])}\right]\\&\leq e^{-st}\prod _{i=1}^{n}e^{\frac {s^{2}(b_{i}-a_{i})^{2}}{8}}\\&=\exp \left(-st+{\tfrac {1}{8}}s^{2}\sum _{i=1}^{n}(b_{i}-a_{i})^{2}\right)\end{aligned}}} $$

    Note that things in the parenthesis are a quadratic function and achieves its minimum at

    $$ {\displaystyle s={\frac {4t}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}.} $$

    Thus we get

    $$ {\displaystyle \operatorname {P} \left(S_{n}-\mathrm {E} \left[S_{n}\right]\geq t\right)\leq \exp \left(-{\frac {2t^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right).} $$


