---
layout: post
title: "CS 7545 Fall 2018 Note"
date: 2018-08-26
timestamp: "2018-09-08 13:15:29 mama"
categories: GT
comments: true
cc: "by-nc-nd"

---

#### From Foundation of Machine Learning (Mohri)

+ A.0 Notation

  We will denote by $$ \mathbb{H} $$ a vector space whose dimension may be infinite.

+ A.1 Definition ***Norms***

  A mapping $$ \| \cdot \| : \mathbb{H} \mapsto \mathbb{R}^+ $$ is said to define a norm if:

  * definiteness: $$ \| \textbf{x} \| = 0 \iff \textbf{x} = 0 $$
  * homogeneity: $$ \forall x \in \mathbb{H}, \forall \alpha \in \mathbb{R},\, \| alp \textbf{x} \| = \left\vert \alpha \right\vert \| \textbf{x} \| $$ 
  * triangle inequality: $$ \forall \textbf{x}, \textbf{y} \in \mathbb{H}, \| \textbf{x} + \textbf{y} \| \leq \| \textbf{x} \| + \| \textbf{y} \| $$.

  My comment: according to [Wikipedia](https://en.wikipedia.org/wiki/Norm_(mathematics)). $$ \mathbb{H} $$ **must be a vector space** over a **subfield** of $$ \mathbb{C} $$. This is required for $$ \left\vert \alpha \right\vert $$ to make sense ( or somehow $$ \mathbb{H} $$ is endowed with an absolute value ) . In **homogeneity** it should also be $$ \alpha \in \mathbb{F} $$ where $$ \mathbb{F} $$ is a subfield of $$ \mathbb{C} $$.

+ p-norm 

  for $$ p \geq 1 $$, the ***p-norm*** of $$ \textbf{x} \in \mathbb{C}^n $$ is defined as

  $$ \| \textbf{x} \|_p = \left( \sum_{j=1}^{n} \left\vert x_j \right\vert ^p \right) ^{1/p} $$

+ equivalency of norms

  two norms $$ \| \cdot \| $$ and $$ \| \cdot \|' $$ are said to be equivalent iff there exists real number $$\alpha, \beta \gt 0 $$ such that $$ \forall \textbf{x} $$

  $$ \alpha \| \textbf{x} \| \leq \| \textbf{x} \|' \leq \beta \| \textbf{x} \| $$

  More generally, all norms on a finite-dimensional space are equivalent.

  My comment: **To be specific**: all norms on a finite-dimensional **Banach** space are equivalent. [Proof](https://math.mit.edu/~stevenj/18.335/norm-equivalence.pdf) ([Cached](/pdfs/norm-equivalence.pdf)). I cannot find for now a counter example of a non-equivalent norm on a non-complete finite-dimensional space.

+ Dual norms

  [see intuition of dual norm here](https://math.stackexchange.com/questions/903484/dual-norm-intuition)([cached](https://web.archive.org/web/20180828200803/https://math.stackexchange.com/questions/903484/dual-norm-intuition))

  Dual is a norm on the dual space

  Let $$ f : V \mapsto \mathbb{F} $$, 

  $$ \| f \|_* = \sup_{x\neq 0}{\frac{ \left\vert f(x) \right\vert} {\| x \|}} $$

  This actually belongs to functional analysis... I don't know...

#### Notes about some Convex

+ B.1 Definition Gradient

  Let $$ f : \mathcal{X} \subset \mathbb{R}^N \mapsto \mathbb{R} $$ be a differentiable function, Then, the gradient of $$ f $$ at $$ x \in \mathcal{X} $$ is the vector in $$ \mathbb{R}^n $$ denoted by $$ \nabla (\textbf{x}) $$ and defined by

  $$ \nabla f(\textbf{x}) = \begin{bmatrix}
    \frac{\partial f}{\partial x_1}(\textbf{x}) \\
    \vdots \\
    \frac{\partial f}{\partial x_n}(\textbf{x})
  \end{bmatrix} $$

+ B.2 Definition Hessian

    Let $$ f : \mathcal{X} \subset \mathbb{R}^N \mapsto \mathbb{R} $$ be a twice differentiable function, Then, the Hession of $$ f $$ at $$ x \in \mathcal{X} $$ is the vector in $$ \mathbb{R}^n $$ denoted by $$ \nabla (\textbf{x}) $$ and defined by

    $$ \nabla^2f(\textbf{x}) = \begin{bmatrix} \frac{\partial^2f}{\partial x_1,x_j}(\textbf{x}) \end{bmatrix} $$

+ Theorem B.1 Fermat's theorem

Let $$ f : \mathcal{X} \subset \mathbb{R}^N \mapsto \mathbb{R} $$ be a differentiable function.
If $$f$$ admits a local extremum at $$ \textbf{x}^* \in \mathcal{X}$$,T
then, $$ \nabla f(\textbf{x}^*) = 0 $$, that is, $$ \textbf{x}^* $$ is a stationary point.

+ B.3 Convex set

  A set $$ \mathcal{X} \in \mathbb{R}^n $$ is said to be convex if for any two points $$ \textbf{x}, \textbf{y} \in \mathcal{X} $$, the segment $$ [\textbf{x}, \textbf{y}] $$ lies in $$ \mathcal{X} $$, that is

  $$ \{ \alpha \textbf{x} + (1-\alpha) \textbf{y} : 0 \leq \alpha \leq 1 \} \subset \mathcal{X} $$

  TODO: <https://xingyuzhou.org/blog/notes/strong-convexity> ([cached](https://web.archive.org/web/20180906131222/https://xingyuzhou.org/blog/notes/strong-convexity))

#### Notes about Fenchel Conjugate

  TODO

#### Notes about some probability.

+ Markov inequality

  $$ P(X\geq a) \leq \frac{\mathbb{E}[x]}{a} $$

  In the language of measure theory, Markov's inequality states that if $$ (X, \Sigma, \mu) $$ is a measure space, f is a measurable extended real-valued function, and $$ \epsilon > 0 $$, then

  $$ μ ( \{ x ∈ X : \left\vert f ( x ) \right\vert ≥ ε \} ) ≤ \frac{1}{\epsilon} \int_X { \left\vert f \right\vert \operatorname{d\mu} }. $$
  
  If $$ \varphi $$ is a monotonically increasing nonnegative function for the nonnegative reals, $$ X $$ is a random variable, $$ a ≥ 0 $$, and $$ \varphi(a) > 0 $$, then

  $$ P ( \left\vert X \right\vert \geq a ) \leq \frac{\mathbb{E}[\varphi(\left\vert X \right\vert )]}{\varphi(a)} $$

  An immediately corollary, using higher moments of **nonnegative** $$X$$ is

  $$ P (X \geq a) \leq \frac{\mathbb{E}[X^n]}{a^n} $$

+ Chebyshev's inequality

  Chebyshev's inequality uses the variance to bound the probability that a random variable deviates far from the mean. Specifically: 

  $$ P( \left\vert X - \mathbb{E}[X] \right\vert \geq a ) \leq \frac{\operatorname{Var}(X)}{a^2},\,\, a > 0 $$

  for which Markov's inequality reads

  $$ {\displaystyle \operatorname {P} \left((X- \mathbb{E}[X])^{2}\geq a^{2}\right)\leq {\frac {\operatorname {Var} (X)}{a^{2}}},} $$

+ Hoeffding's Lemma

  Let $$ X $$ be any real-valued random variable with expected value $$ {\displaystyle \mathbb {E} (X)=0} $$
  and such that $$ {\displaystyle a\leq X\leq b} $$ almost surely. Then, for all $$ {\displaystyle \lambda \in \mathbb {R} } $$,

  $$ {\displaystyle \mathbb {E} \left[e^{\lambda X}\right]\leq \exp \left({\frac {\lambda ^{2}(b-a)^{2}}{8}}\right).} $$

  Note that because of the assumption that the random variable $$ X $$ has zero expectation, the $$ a $$ and $$ b $$ in the lemma must satisfy $$ a\leq 0\leq b $$

  <details><summary>Show Proof</summary><p>

  First note that if one of $$a$$ or $$b$$ is zero, then $$ {\displaystyle \textstyle \mathbb {P} \left(X=0\right)=1} $$ and the inequality follows. If both are nonzero, then $$ a $$ must be negative and $$ b $$ must be positive.

  Next, since that $$ {\displaystyle e^{sx}} $$ is a convex function on the real line:

  $$ \forall x \in [a, b]: \,\, e^{sx}\leq \frac{b-x}{b-a}e^{sa}+\frac{x-a}{b-a}e^{sb}. $$

  Applying $$ \mathbb {E} $$ to both sides of the above inequality gives us:

  $$ {\displaystyle {\begin{aligned}\mathbb {E} \left[e^{sX}\right]&\leq {\frac {b-\mathbb {E} [X]}{b-a}}e^{sa}+{\frac {\mathbb {E} [X]-a}{b-a}}e^{sb}\\&={\frac {b}{b-a}}e^{sa}+{\frac {-a}{b-a}}e^{sb}&&\mathbb {E} (X)=0\\&=(1-\theta )e^{sa}+\theta e^{sb}&&\theta =-{\frac {a}{b-a}}>0\\&=e^{sa}\left(1-\theta +\theta e^{s(b-a)}\right)\\&=\left(1-\theta +\theta e^{s(b-a)}\right)e^{-s\theta (b-a)}\\\end{aligned}}} $$

  Let $$ u=s(b-a) $$ and define $$ \varphi :\mathbb {R} \mapsto \mathbb {R} $$ :

  $$ \varphi (u)=-\theta u+\log \left(1-\theta +\theta e^{u}\right) $$

  $$ \varphi $$ is well defined on $$ \mathbb{R} $$, to see this we calculate:

  $$ {\displaystyle {\begin{aligned}1-\theta +\theta e^{u}&=\theta \left({\frac {1}{\theta }}-1+e^{u}\right)\\&=\theta \left(-{\frac {b}{a}}+e^{u}\right)\\&>0&&\theta >0,\quad {\frac {b}{a}}<0\end{aligned}}} $$

  The definition of $$ \varphi $$ implies

  $$  \mathbb {E} \left[e^{sX}\right]\leq e^{\varphi (u)}. $$

  By Taylor's theorem, for every real $$ u $$ there exists a $$ v $$ between $$ {\displaystyle 0} $$ and $$ u $$ such that

  $$ \varphi(u)=\varphi(0)+u\varphi'(0)+\tfrac{1}{2} u^2\varphi''(v). $$

  Note that:

  $$ {\displaystyle {\begin{aligned}\varphi (0)&=0\\\varphi '(0)&=-\theta +\left.{\frac {\theta e^{u}}{1-\theta +\theta e^{u}}}\right|_{u=0}\\&=0\\[6pt]\varphi ''(v)&={\frac {\theta e^{v}\left(1-\theta +\theta e^{v}\right)-\theta ^{2}e^{2v}}{\left(1-\theta +\theta e^{v}\right)^{2}}}\\[6pt]&={\frac {\theta e^{v}}{1-\theta +\theta e^{v}}}\left(1-{\frac {\theta e^{v}}{1-\theta +\theta e^{v}}}\right)\\[6pt]&=t(1-t)&&t={\frac {\theta e^{v}}{1-\theta +\theta e^{v}}}\\&\leq {\tfrac {1}{4}}&&t>0\end{aligned}}} $$

  Therefore,

  $$ {\displaystyle \varphi (u)\leq 0+u\cdot 0+{\tfrac {1}{2}}u^{2}\cdot {\tfrac {1}{4}}={\tfrac {1}{8}}u^{2}={\tfrac {1}{8}}s^{2}(b-a)^{2}.} $$

  This implies

  $$ {\displaystyle \mathbb {E} \left[e^{sX}\right]\leq \exp \left({\tfrac {1}{8}}s^{2}(b-a)^{2}\right).} $$

  </p></details>

+ Hoeffding's inequality

  Let $$ X_1, \ldots, X_n $$ be independent random variables bounded by the interval $$ [0, 1]: 0 ≤ X_i ≤ 1 $$. We define the empirical mean of these variables by

  $$ {\displaystyle {\overline {X}}={\frac {1}{n}}(X_{1}+\cdots +X_{n}).} $$

  One of the inequalities in Theorem 1 of Hoeffding (1963) states

  $$ {\displaystyle {\begin{aligned}\operatorname {P} ({\overline {X}}-\mathrm {E} [{\overline {X}}]\geq t)\leq e^{-2nt^{2}}\end{aligned}}} $$

  where $$ {\displaystyle 0\leq t}. $$

  Theorem 2 of Hoeffding (1963) is a generalization of the above inequality when it is known that $$ X_i $$ are strictly bounded by the intervals $$ [a_i, b_i] $$:

  $$ {\displaystyle {\begin{aligned}\operatorname {P} \left({\overline {X}}-\mathrm {E} \left[{\overline {X}}\right]\geq t\right)&\leq \exp \left(-{\frac {2n^{2}t^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right)\\\operatorname {P} \left(\left|{\overline {X}}-\mathrm {E} \left[{\overline {X}}\right]\right|\geq t\right)&\leq 2\exp \left(-{\frac {2n^{2}t^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right)\end{aligned}}} $$

  <details><summary>Show Proof</summary><p>

  Suppose $$ X_1,\ldots,X_n $$ are n independent random variables such that

  $$ {\displaystyle \operatorname {P} \left(X_{i}\in [a_{i},b_{i}]\right)=1,\qquad 1\leq i\leq n.} $$

  Let $$ {\displaystyle S_{n}=X_{1}+\cdots +X_{n}.} $$

  Then for $$ s, t ≥ 0 $$, Markov's inequality and the independence of $$X_i$$ implies:

  $$ {\displaystyle {\begin{aligned}\operatorname {P} \left(S_{n}-\mathrm {E} \left[S_{n}\right]\geq t\right)&=\operatorname {P} \left(e^{s(S_{n}-\mathrm {E} \left[S_{n}\right])}\geq e^{st}\right)\\&\leq e^{-st}\mathrm {E} \left[e^{s(S_{n}-\mathrm {E} \left[S_{n}\right])}\right]\\&=e^{-st}\prod _{i=1}^{n}\mathrm {E} \left[e^{s(X_{i}-\mathrm {E} \left[X_{i}\right])}\right]\\&\leq e^{-st}\prod _{i=1}^{n}e^{\frac {s^{2}(b_{i}-a_{i})^{2}}{8}}\\&=\exp \left(-st+{\tfrac {1}{8}}s^{2}\sum _{i=1}^{n}(b_{i}-a_{i})^{2}\right)\end{aligned}}} $$

  Note that things in the parenthesis are a quadratic function and achieves its minimum at

  $$ {\displaystyle s={\frac {4t}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}.} $$

  Thus we get

  $$ {\displaystyle \operatorname {P} \left(S_{n}-\mathrm {E} \left[S_{n}\right]\geq t\right)\leq \exp \left(-{\frac {2t^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right).} $$

+ Bregman Divergence

  Let $$ F:\Omega \to \mathbb {R} $$ be a continuously-differentiable, strictly convex function defined on a closed convex set $$ \Omega $$.

  The Bregman distance associated with F for points $$ p,q\in \Omega $$ is the difference between the value of F at point p and the value of the first-order Taylor expansion of F around point q evaluated at point p:

  $$ D_{F}(p,q)=F(p)-F(q)-\langle \nabla F(q),p-q\rangle $$

+ Martingale sequence

   A basic definition of a discrete-time martingale is a discrete-time stochastic process (i.e., a sequence of random variables) $$ X_1, X_2, X_3, \ldots $$ that satisfies for any time $$n$$,

   $$
   \begin{aligned}
     & \mathbf {E} (\vert X_{n}\vert )<\infty \\
     & \mathbf {E} (X_{n+1}\mid X_{1},\ldots ,X_{n})=X_{n}
   \end{aligned}
   $$

   That is, the conditional expected value of the next observation, given all the past observations, is equal to the most recent observation.

+ Azuma's inequality

  Suppose $$ \{ X_k : k = 0, 1, 2, 3, \ldots \} $$ is a martingale (or super-martingale) and

  $$ {\displaystyle |X_{k}-X_{k-1}|<c_{k},\,} $$

  almost surely. Then for all positive integers $$N$$ and all positive reals $$t$$,

  $$ {\displaystyle P(X_{N}-X_{0}\geq t)\leq \exp \left({-t^{2} \over 2\sum _{k=1}^{N}c_{k}^{2}}\right).} $$

  TODO: Proof
