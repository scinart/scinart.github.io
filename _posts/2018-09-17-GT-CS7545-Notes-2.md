---
layout: post
title: "CS 7545 Fall 2018 Notes 2"
date: 2018-09-17
timestamp: "2018-09-19 23:11:55 mama"
categories: GT
comments: true
cc: "by-nc-nd"

---

## Game Theory

Def: A two player (finite strategy) game is given by a pair of matrices

$$
N \in \mathbb{R}^{n\times m}, M \in \mathbb{R}^{n\times m}
$$

where

$$
M_{i,j} = \text{payoff to player 1 if } p_1 \text{ selects action } i \text{ and } p_2 \text{ selects action } j
$$

Let's draw $$ M $$ here

$$
M = \begin{bmatrix}
  m_{1,1} & \cdots & m_{1_m} \\
  m_{2,1} & \cdots & m_{2_m} \\
  m_{3,1} & \cdots & m_{3_m} \\
  m_{4,1} & \cdots & m_{4_m} \\
  m_{5,1} & \cdots & m_{5_m} \\
  \vdots & \vdots & \vdots \\
  m_{n,1} & \cdots & m_{n_1} \\
\end{bmatrix}
$$

Note: $$ \textbf{p}^T M \textbf{q} $$ is the expected gain of player 1 if $$p_i$$ is probability of prayer 1 taking action $$i$$ and $$q_j$$ is the probability of player 2 taking action $$j$$


Def: A game is zero sum if

$$ N = -M $$

Def: A Nash equilibrium is a pair $$\widetilde{p} \in \Delta_n, \widetilde{q} \in \Delta_m, $$ s.t.

$$
\forall p \in \Delta_n, \widetilde{p}^T M \widetilde{q} \geq p^T M \widetilde{q}
$$


$$
\forall q \in \Delta_m, \widetilde{p}^TN\widetilde{q} \geq \widetilde{p}^TNq
$$

Note

$$
p^T M q = \mathbb{E}_p \mathbb{E}_q{M_{i,j}}
$$

Nash's theorem: There exist a (possibly non-unique) Nash equilibrium for any 2-player game.

Von Neumann's min-max theorem:

$$ ∀M \in \mathbb{R}^{n× m}, $$

$$ \min_{p\in \Delta_n} \max_{q\in \Delta_m} p^T M q = \max_{q\in \Delta_m} \min_{p\in \Delta_n} p^T M q  $$

We say that an algorithm $$ \mathcal{A} $$ is no-regret if $$ \forall \ell_1 \ldots \ell_T \ldots \in [0,1] $$ with $$p_t \in \Delta_n $$ chosen as $$ p_t ⟵ \mathcal{A}(\ell_1,\ldots,\ell_{t-1}) $$

Then

$$
\frac{1}{T} \left( \sum_{t=1}^T{\textbf{p}^t \cdot \boldsymbol{\ell}^t} - \min_{p\in \Delta_n}{\sum_{t=1}^{T}{\textbf{p}^t \cdot \boldsymbol{\ell}^t}} \right) = \epsilon_T = O(1)
$$

Observe:

$$ \min_{p\in \Delta_n}{\sum_{t=1}^{T}{\textbf{p}^t \cdot \boldsymbol{\ell}_t}} = \min_{i=1\ldots n}{\sum_{t=1}^{T}{\textbf{e}_i \cdot \boldsymbol{\ell}^t}} $$

Claim:

EWA is as no-regret algorithm with $$ \epsilon_T \leq \frac{\log N + \sqrt{2 T \log N}}{T} = \frac{\log N}{T} + \sqrt{\frac{2\log N}{T}} $$

-----------

No regret algorithm performs well in most adversary case

Let $$ M $$ be $$ \mathbb{R}^{n\times m}, M_{i,j} \in \{0, 1\} \;\; \Why M_{i,j} \in \{0, 1\} $$

Let $$ \mathcal{A} $$ be a no-regret algorithm, we will play this game repeatedly.

Imagine the following protocol: For $$ t = 1 \ldots T $$,

$$ p_t $$ is chosen as $$ \mathcal{A}(\ell_1,\ldots,\ell_{t-1}), \text{ where } \ell_s = Mq_s (s = 1\ldots t-1) $$

$$ q_t $$ is chosen as $$ q_t = \operatorname*{argmax}_{q\in \Delta_m}{p_t^TMq}\;\;\;\; \text{ a.k.a.most adversary nature} $$

Q1: How happy is q

$$
\begin{aligned}
  \frac{1}{T}\sum_{t=1}^{T}{\textbf{p}^t \cdot \textbf{M} \textbf{q}^t } &= \frac{1}{T} \sum_{t=1}^{T}{\max_{\textbf{q}}\textbf{p}^t\cdot \textbf{M} \textbf{q}^t} \\
  &≥ \frac{1}{T}\max_{\textbf{q}}{\sum_{t=1}^{T}{(\textbf{p}^t \cdot \textbf{M} \textbf{q})}} \\
  &= \frac{1}{T}\max_{\textbf{q}}{\sum_{t=1}^{T}{(\textbf{p}^t)}} \cdot \textbf{M} \textbf{q} = \max_{\textbf{q}}{ \bar{\textbf{p}} } \cdot \textbf{M} \textbf{q} \\
  &≥ \min_{\textbf{p}}\max_{\textbf{q}} \textbf{p}^t\cdot \textbf{M} \textbf{q}
\end{aligned}
$$

Q2: How happy is p

$$
\begin{aligned}
  \frac{1}{T}\sum_{t=1}^{T}{\textbf{p}^t \cdot \textbf{M} \textbf{q}^t} &= \frac{1}{T}\sum_{t=1}^{T}{\textbf{p}^t \cdot \boldsymbol{\ell}^t} & \\
  &= \frac{1}{T}\min_{\textbf{p}}{\sum_{t=1}^{T}{\textbf{p}^t\cdot \boldsymbol{\ell}^t}} + \epsilon_T & \text{ by definition of no regret} \\
  &= \min_{\textbf{p}}{\frac{1}{T} \sum_{t=1}^{T}{\textbf{p}^t \cdot \textbf{M} \textbf{q}^t}} & \\
  &= \min_{\textbf{p}}{\textbf{p}^t \cdot \textbf{M} \bar{\textbf{q}}} & \\
  &≤ \max_{\textbf{q}} \min_{\textbf{p}} \textbf{p}^t \cdot \textbf{M} \textbf{q} + \epsilon_T
\end{aligned}
$$

Corollary:

$$ \bar{\textbf{p}} $$ and $$ \bar{\textbf{q}} $$ are $$ \epsilon_T $$-optimal Nash eq.

<hr>

Given $$ \textbf{x}_1,\ldots,\textbf{x}_n \in \mathcal{X} $$, $$ \textbf{y}_1,\ldots, \textbf{y}_n \in \{0,1\} $$

Hypothesis class $$ H = \{ h_1,\ldots,h_m \} $$ where $$ h_i : \mathcal{X} \mapsto \{ -1, 1 \} $$

+ Weak learning hypothesis

  if $$ ∀ \textbf{p} \in \Delta_n,\, ∃ h \in H,\,\text{s.t.} $$ if $$ \textbf{x}_i $$ show up with probability $$ p_i $$, then

  $$ \operatorname{Pr}\{ h(\textbf{x}_i) \neq y_i \} \leq \frac{1}{2} - \frac{\gamma}{2},\;\; \gamma > 0 $$

  which is equivalent to

  $$ \gamma \leq \sum_{i}{p_iy_ih_i(\textbf{x}_i)} $$

  then there exist $$ \textbf{q} \in \Delta_m $$, s.t. $$ ∀ i = 1\ldots n,\ $$

  $$
    \sum_{h\in H}{\textbf{q}_h \cdot h(\textbf{x}_i) y_i} > 0
  $$

  Define $$ \textbf{M} \in \{ -1, 1 \}^{n×m} $$, $$ \textbf{M}_{i,j} = h_j(\textbf{x}_i)y_i $$, then

  $$ \gamma \leq \sum_{i}{p_iy_ih_i(\textbf{x}_i)} = \textbf{p}^T \textbf{M} \textbf{e}_j $$

  Because this holds for any $$ \textbf{p} $$, we have

  $$ \gamma \leq \min_{\textbf{p} \in \Delta_n}{\textbf{p}^T \textbf{M} \textbf{e}_j} \leq \min_{\textbf{p} \in \Delta_n}\max_{\textbf{q} \in \Delta_h}{\textbf{p}^T \textbf{M} q} $$

  So

  $$ 0 \leq \max_{\textbf{q} \in \Delta_h}\min_{\textbf{p} \in \Delta_n}{\textbf{p}^T \textbf{M} q} \;\;\;\;\text{greater than gamma??} $$



