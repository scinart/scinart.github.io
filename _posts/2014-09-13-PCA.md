---
layout: post
title: "Pricipal Component Analysis 主成分分析"
date: 2014-09-13
timestamp: "2014-09-15 09:20:51 scinart"
categories: math
tag: [note, ml]
comments: true
toc: false
cc: "by-nc-sa"

---

### 问题描述

给定 $$n$$ 维空间中的点 $$m$$ 个，求一$$k\,(k<n)$$维超平面，使得将 $$m$$ 个点投影到超平面后的**损失最小**。

### 背景知识

+ 矩阵是一种线性变换
+ 特征向量是线性变换中方向不变的向量，即 $$\mathbf{Ax}=λ\mathbf{x}$$，λ是特征值
+ 随机变量 $$X$$ 方差 $$σ^2=Var(X)$$ 代表了 $$X$$ 分布的离散程度，$$Var(X)=E[(X-μ)^2]$$
+ 样本方差 $$S^2=\frac{1}{m-1}\sum_{i=1}^{m}{\left(X_{i}-\bar{X}\right)^2}$$ 是[总体方差的一个无偏估计](http://www.zhihu.com/question/20099757)。
+ [拉格朗日乘数法](https://www.youtube.com/watch?v=15HVevXRsBA)

### PCA

这里**损失最小**可以定义为：

1. 样本在投影方向上的方差最大。
2. 样本与投影之后的点的距离和最小。

事实上，这2个最优化目标是殊途同归的。我还不知道为什么，所以觉得很神奇。

在处理数据之前，通常要对它进行标准化，a.k.a, substract each sample by their mean. 这是因为：

1. 标准化本质是线性变换(平移)，所以不过改变投影的任何性质
2. 正是由于1，所以我们即使从原问题出发，找到超平面，那么这个超平面沿与这个超平面垂直的方向平移时，投影的相对位置没有改变，样本与投影之后的距离会变化，可以轻易地证明，当这个超平面经过样本的平均点时，平方误差最小！

所以，在下面的讨论中我们认为样本的均值都是0。

#### 1. 样本在投影方向上的方差最大。

方差最大？这只是一种直观的理解，样本投影之后有$$k$$个维度，何为最大！
这里明确的说明，是**方差和最大**，为什么是方差和最大而不是其他一些函数如平方和？
原因：其一，这只是直观理解，我目前没有直观理由说服你不用平方和。
其二，方差和最大同时意味着**样本与投影之后的点的距离和最小**。
所以这只是下面要证明的一个步骤。

设投影方向为单位向量 $$\mathbf{μ}$$，第 $$i$$ 个样本为 $$\mathbf{x}^{(i)}$$，

样本投影后变成 $$ \mathbf{x}⋅\mathbf{μ} $$，易知投影后均值还是0。

投影后方差为

$$
\begin{align}
\frac{1}{m-1}\sum_{i=1}^{m}{( {\mathbf{x}^{i}}^{T}\mathbf{μ})^2} &= \frac{1}{m-1}\sum_{i=1}^{m}{\mathbf{μ}^T \mathbf{x}^{(i)} {\mathbf{x}^{(i)}}^{T} \mathbf{μ}} \\
&= \mathbf{μ}^{T}\left(\frac{1}{m-1}\sum_{i=1}^{m}{\mathbf{x}^{(i)} {\mathbf{x}^{(i)}}^{T}}\right)\mathbf{μ}
\end{align}
$$

其中 $$ \left(\frac{1}{m-1}\sum_{i=1}^{m}{\mathbf{x}^{(i)} {\mathbf{x}^{(i)}}^{T}}\right) $$ 为样本的协方差矩阵，记为 $$Σ$$。

于是，问题变成了，

$$ \text{maximize } \mathbf{μ}^T \mathbf{Σμ} \text{ constrained by } \mathbf{μ}^T \mathbf{μ}=1 $$

此时，拉格朗日乘数法光荣上场。

我们定义$$f = \mathbf{μ}^T \mathbf{Σμ},\,g = \mathbf{μ}^T \mathbf{μ}$$，则

$$
\begin{align}
∇f &= λ∇g \\
∇({\mathbf{μ}^T \mathbf{Σμ}}) &= λ(∇\mathbf{μ}^T \mathbf{μ}) \\
\mathbf{Σμ} + (\mathbf{μ}^T\mathbf{Σ})^T &= λ(\mathbf{μ}+{\mathbf{μ}^{T}}^T) \\
\mathbf{Σμ} &= λ\mathbf{μ}
\end{align}
$$

这里居然出现了协方差矩阵的特征值！！尼玛啊。微积分线性代数和概率论结合的太美那画面我不敢看。

$$ f = \mathbf{μ}^T \mathbf{Σμ} = λ $$

于是，当$$k=1$$时，我们显然取$$\mathbf{Σ}$$的最大的特征值

当$$k>=2$$时，直觉告诉我们应该取第二大的特征值，但是直的是这样么？

事实真的是这样，但是其证明远超出作者本人的理解范围。其证明可见[第五章矩阵函数及其微积分 - 上海交通大学数学系](math.sjtu.edu.cn/course/Matrix/Chapter5.pdf) (这个讲义的引言与该例子一模一样)

总之，结论就是取前k个特征值和前k个特征向量。

#### 2. 样本与投影之后的点的距离和最小。

假设新基是 $$\mathbf{u}_1,\,\mathbf{u}_2,\,\cdots,\,\mathbf{u}_k$$，即$$\mathbf{U}$$是一个$$n×k$$的矩阵
设 $$\mathbf{x}^{(i)} = \mathbf{U}\mathbf{y}^{(i)}$$，
即$$\mathbf{x}^{(i)}$$在新基下的坐标是$$\mathbf{y}_{1}^{(i)},\,\mathbf{y}_{2}^{(i)},\,\cdots\,\mathbf{y}_{k}^{(i)}$$


$$
\text{Fact 1}\\
E \lVert \mathbf{X} \rVert^2 = E \lVert \hat{\mathbf{X}} \rVert ^2 + E \lVert \mathbf{X}-\hat{\mathbf{X}} \rVert ^2 \\
\text{证：对每个样本x} \\
\lVert \mathbf{x} \rVert^2 = \lVert\hat{\mathbf{x}}\rVert^2+\lVert\mathbf{x}-\hat{\mathbf{x}}\rVert^2 \\
\text{取期望即可}
$$

$$
\text{Fact 2}\\
E\lVert\hat{\mathbf{X}}\rVert^2 = \sum_{i=1}^{k}{E(\mathbf{Y}_{i}^2)} \\
\text{证：对每个样本x} \\
\hat{x}=\sum_{i=1}^{k}{y_i u_i} \\
\lVert\hat{\mathbf{x}}\rVert^2 = \sum_{i=1}^{k}{y_{i}^2} \\
\text{取期望即可}
$$

如果样本均值为0，那么

$$
{E(\mathbf{Y}_{i}^2)} = Var(\mathbf{Y}_i) \\
E\lVert\hat{\mathbf{X}}\rVert^2 = \sum_{i=1}^{k}{Var(\mathbf{Y}_i)} \\
E \lVert \mathbf{X}-\hat{\mathbf{X}} \rVert ^2 = E \lVert \mathbf{X} \rVert^2-\sum_{i=1}^{k}{Var(\mathbf{Y}_i)}
$$

Since E X 2 is fixed, minimizing the mean square error is equivalent to maximizing the variance of the components.

### In Practice

+ Preprocessing	(feature scaling/mean normalization)
+ Compute "covariance matrix":

  $$
    Σ = \frac{1}{m}{\sum_{i=1}^{n}{(\mathbf{x}^{(i)})(\mathbf{x}^{(i)})^T}}
  $$
  机器学习通常用$$\frac{1}{m}$$而不是$$\frac{1}{m-1}$$。
+ Compute "eigenvectors" of matrix Σ:

  $$
    [U,S,V] = svd(Σ);
  $$
  此时S是包含Σ特征值的对角矩阵(特征值从大到小)，U和V相同，都是对应的特征向量。这是因为协方差矩阵符合**symmetric positive definite**.
+ Ur = U(:,1:k); -- Ur means U reduced.
+ $$z = Ur^T * x$$;
+ 复原用$$\hat{x} = Ur*z$$

  这就很好理解了。$$Ur^T*x$$相当于取内积，$$Ur(Ur^T Ur)^{-1}Ur^T = Ur Ur^T$$是投影矩阵！
+ Choosing k (number of principal components)

  Typically, choose k to smallest value so that

  $$
     \frac{\frac{1}{1}\sum_{i=1}^{m}{\lVert x^{(i)}-\hat{x}^{(i)}\rVert^2}}{\frac{1}{m}\sum_{i=1}^{m}{\lVert x^{(i)}\rVert^2}} ≤ 0.01\text{ (or 0.05 ...)}
  $$

  which means that 99%(95% ...) of variance is retained.

### See:

[Tutorial on Principal Component Analysis](http://mplab.ucsd.edu/tutorials/pca.pdf) by JR Movellan  
[第五章矩阵函数及其微积分 - 上海交通大学数学系](math.sjtu.edu.cn/course/Matrix/Chapter5.pdf)  
Machine Learning by Andrew Ng on [Coursera.org](https://www.coursera.org/)

