---
layout: post
title: "PLA: Perceptron Learning Algorithm"
date: 2014-04-06 
timestamp: "2014-06-15 22:39:43 scinart"
categories: cs
tag: note
comments: true
toc:
cc: "by-nc-sa"

---

### PLA Algorithm.

#### Input:

* A set of linear separable points $$ \mathbf{x_{1} \cdots x_{n}},$$ their group $$ y_1 \cdots y_n,\, y_i = ±1. $$

#### Output:

* A vector $$ \mathbf{w_g} $$ that $$ y_i \mathbf{w_g^{T}}\mathbf{x_i}>0 \text{ for each x_i}$$

#### Algorithm:

$$
\begin{array}{l}
\mathbf{w_0} = \mathbf{0} \\
\text{t = 0} \\
\text{while there exists } \mathbf{x_i} \text{ s.t. } y_i\mathbf{w_{t}^{T}}\mathbf{x_i}<0 \\
\quad\mathbf{w_{t+1}}=\mathbf{w_t}+y_i\mathbf{x_i} \\
\quad\text{t++} \\
\text{return }\mathbf{w_t}
\end{array}
$$

#### Correctness: 

assume after t times iteration, $$\mathbf{x_{n_1}}, \mathbf{x_{n_2}}, \cdots, \mathbf{x_{n_t}}$$ has been chosen.

Consider the angle between $$ \mathbf{w_t} $$ and the ideal $$ \mathbf{w_f} $$

$$

\begin{aligned}
\frac{\mathbf{w_f^T}}{\|\mathbf{w_f}\|}\frac{\mathbf{w_t}}{\|\mathbf{w_t}\|}
=& \frac{1}{\|\mathbf{w_f}\|}\frac{1}{\|\mathbf{w_t}\|}\sum_{i=1}^{t}{y_{n_i}\mathbf{w_f^T}\mathbf{x_{n_i}}} \\
≥& \frac{1}{\|\mathbf{w_f}\|}\frac{1}{\|\mathbf{w_t}\|}t⋅\min_{i=1}^{t\,or\,n}{y_i\mathbf{w_f^T}\mathbf{x_i}} \\
=& \frac{1}{\|\mathbf{w_t}\|}t⋅ρ\quad\text{where ρ is }\frac{\min_{i=1}^{n}{y_i\mathbf{w_f^T}\mathbf{x_i}}}{\|\mathbf{w_f}\|} \\
≥& \frac{1}{\sqrt{t}\mathbf{R}}t⋅ρ\quad \text{about R, see below.} \\
=& \sqrt{t}⋅\frac{ρ}{\mathbf{R}}
\end{aligned}
$$

$$
\begin{aligned}
\|\mathbf{w_{t+1}}\|^2 &=\|\mathbf{w_t}+y_{n(t)}\mathbf{x_{n(t)}}\|^2 \\
                       &=\|\mathbf{w_t}\|^2 + \color{blue}{2y_{n(t)}\mathbf{w_t^Tx_{n(t)}}}+\|y_{n(t)}\mathbf{x_{n(t)}}\|^2 \\
					   &≤\|\mathbf{w_t}\|^2+\color{blue}{0}+\color{red}{\|y_{n(t)}\mathbf{x_{n(t)}}\|}^2 \\
					   &≤\|\mathbf{w_t}\|^2+\color{red}{\max_{n}{\|\color{grey}{y_n}\mathbf{x_n}}\|^2} \\
∴ \|\mathbf{w_t}\|^2  &≤t{\max_{n}{\|\mathbf{x_n}}\|^2} = t\mathbf{R}^2 \quad\text{where }\mathbf{R}\text{ = }{\max_{n}{\|\mathbf{x_n}}\|}
\end{aligned}
$$

Hence this algorithm will stop at most $$ \left\lceil \frac{\mathbf{R}^2}{ρ^2} \right\rceil $$ times iteration.

#### See:

This article [Perceptron Learning Algorithm (PLA)](http://beader.me/2013/12/21/perceptron-learning-algorithm/) gives a very detailed example.  
Coursera [機器學習基石 (Machine Learning Foundations)](https://www.coursera.org/course/ntumlone)
